{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has shape: (2565, 128)\n",
      "y has shape: (2565, 1)\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "data_path = '../data/processed/'\n",
    "X = np.load(data_path + 'X.npy')\n",
    "y = np.load(data_path + 'y.npy')\n",
    "print(\"X has shape: {}\\ny has shape: {}\".format(X.shape, y.shape))\n",
    "X_torch = torch.from_numpy(X).float()\n",
    "y_torch = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9375e-11]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src/models/')\n",
    "from linear_nn import three_layer_nn, fro_loss\n",
    "\n",
    "s = 1e-1\n",
    "model = three_layer_nn('normal', s, False, p=0.1)\n",
    "loss_fn = fro_loss()\n",
    "\n",
    "cross_cov = (1 / len(y_torch)) * y_torch.transpose(0,1) @ X_torch # (1, 128)\n",
    "y_cov = (1 / len(y_torch)) * y_torch.transpose(0,1) @ y_torch\n",
    "global_opt = -0.5 * (cross_cov @ cross_cov.transpose(0,1)) + 0.5 * y_cov # Taken from appendix A in paper \n",
    "global_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12397629022598267\n",
      "1 0.08094190061092377\n",
      "2 0.0576319694519043\n",
      "3 0.043488673865795135\n",
      "4 0.03422371670603752\n",
      "5 0.02779902145266533\n",
      "6 0.02314072474837303\n",
      "7 0.019639352336525917\n",
      "8 0.01692858338356018\n",
      "9 0.014777785167098045\n",
      "10 0.013035918585956097\n",
      "11 0.011600688099861145\n",
      "12 0.010400698520243168\n",
      "13 0.009384789504110813\n",
      "14 0.008515425026416779\n",
      "15 0.0077644879929721355\n",
      "16 0.007110513746738434\n",
      "17 0.006536843255162239\n",
      "18 0.006030356977134943\n",
      "19 0.005580582655966282\n",
      "20 0.0051790690049529076\n",
      "21 0.004818921443074942\n",
      "22 0.004494460299611092\n",
      "23 0.004200970288366079\n",
      "24 0.0039345077238976955\n",
      "25 0.003691749181598425\n",
      "26 0.0034698783420026302\n",
      "27 0.003266492858529091\n",
      "28 0.0030795345082879066\n",
      "29 0.0029072295874357224\n",
      "30 0.0027480425778776407\n",
      "31 0.002600638894364238\n",
      "32 0.0024638555478304625\n",
      "33 0.0023366697132587433\n",
      "34 0.002218186156824231\n",
      "35 0.0021076109260320663\n",
      "36 0.002004244364798069\n",
      "37 0.0019074634183198214\n",
      "38 0.0018167129019275308\n",
      "39 0.001731496537104249\n",
      "40 0.001651371014304459\n",
      "41 0.0015759369125589728\n",
      "42 0.0015048350905999541\n",
      "43 0.0014377402840182185\n",
      "44 0.0013743597082793713\n",
      "45 0.0013144270051270723\n",
      "46 0.001257699797861278\n",
      "47 0.0012039579451084137\n",
      "48 0.0011529999319463968\n",
      "49 0.0011046420549973845\n",
      "50 0.0010587152792140841\n",
      "51 0.0010150650050491095\n",
      "52 0.0009735494968481362\n",
      "53 0.0009340372635051608\n",
      "54 0.0008964078151620924\n",
      "55 0.0008605491602793336\n",
      "56 0.0008263583877123892\n",
      "57 0.0007937396294437349\n",
      "58 0.0007626042352057993\n",
      "59 0.0007328694337047637\n",
      "60 0.0007044586818665266\n",
      "61 0.0006773008499294519\n",
      "62 0.0006513286498375237\n",
      "63 0.0006264800322242081\n",
      "64 0.0006026969058439136\n",
      "65 0.0005799248465336859\n",
      "66 0.000558112864382565\n",
      "67 0.00053721311269328\n",
      "68 0.0005171805969439447\n",
      "69 0.000497973058372736\n",
      "70 0.0004795512359123677\n",
      "71 0.00046187714906409383\n",
      "72 0.00044491616426967084\n",
      "73 0.0004286350158508867\n",
      "74 0.0004130021552555263\n",
      "75 0.0003979879547841847\n",
      "76 0.0003835648240055889\n",
      "77 0.0003697059291880578\n",
      "78 0.0003563867066986859\n",
      "79 0.0003435828839428723\n",
      "80 0.00033127248752862215\n",
      "81 0.00031943406793288887\n",
      "82 0.0003080471360590309\n",
      "83 0.00029709277441725135\n",
      "84 0.0002865528513211757\n",
      "85 0.0002764094388112426\n",
      "86 0.000266646413365379\n",
      "87 0.0002572482917457819\n",
      "88 0.00024819979444146156\n",
      "89 0.00023948671878315508\n",
      "90 0.00023109545873012394\n",
      "91 0.00022301300487015396\n",
      "92 0.00021522720635402948\n",
      "93 0.00020772614516317844\n",
      "94 0.00020049851445946842\n",
      "95 0.00019353360403329134\n",
      "96 0.0001868210092652589\n",
      "97 0.00018035109678748995\n",
      "98 0.0001741143350955099\n",
      "99 0.0001681017893133685\n",
      "100 0.0001623048010515049\n",
      "101 0.00015671509027015418\n",
      "102 0.00015132494445424527\n",
      "103 0.00014612663653679192\n",
      "104 0.00014111303607933223\n",
      "105 0.0001362771145068109\n",
      "106 0.00013161217793822289\n",
      "107 0.00012711186718661338\n",
      "108 0.00012277015775907785\n",
      "109 0.00011858116340590641\n",
      "110 0.00011453916522441432\n",
      "111 0.00011063867714256048\n",
      "112 0.00010687470057746395\n",
      "113 0.0001032421860145405\n",
      "114 9.973628766601905e-05\n",
      "115 9.635245078243315e-05\n",
      "116 9.308598237112164e-05\n",
      "117 8.993299707071856e-05\n",
      "118 8.688931848155335e-05\n",
      "119 8.395100303459913e-05\n",
      "120 8.111405622912571e-05\n",
      "121 7.837510202080011e-05\n",
      "122 7.573055336251855e-05\n",
      "123 7.317713607335463e-05\n",
      "124 7.071124855428934e-05\n",
      "125 6.833022780483589e-05\n",
      "126 6.603081419598311e-05\n",
      "127 6.38102283119224e-05\n",
      "128 6.166566163301468e-05\n",
      "129 5.9594436606857926e-05\n",
      "130 5.759390478488058e-05\n",
      "131 5.5661730584688485e-05\n",
      "132 5.379538197303191e-05\n",
      "133 5.199253428145312e-05\n",
      "134 5.025108839618042e-05\n",
      "135 4.856887608184479e-05\n",
      "136 4.694376184488647e-05\n",
      "137 4.5373857574304566e-05\n",
      "138 4.385714782983996e-05\n",
      "139 4.239190093358047e-05\n",
      "140 4.097620694665238e-05\n",
      "141 3.960840331274085e-05\n",
      "142 3.828696208074689e-05\n",
      "143 3.701007517520338e-05\n",
      "144 3.577632378437556e-05\n",
      "145 3.4584187233122066e-05\n",
      "146 3.343219941598363e-05\n",
      "147 3.231911250622943e-05\n",
      "148 3.1243445846484974e-05\n",
      "149 3.0204055292415433e-05\n",
      "150 2.919953658420127e-05\n",
      "151 2.8228865630808286e-05\n",
      "152 2.7290769139654003e-05\n",
      "153 2.638417026901152e-05\n",
      "154 2.5507975806249306e-05\n",
      "155 2.4661138013470918e-05\n",
      "156 2.384275103395339e-05\n",
      "157 2.3051808966556564e-05\n",
      "158 2.228727680630982e-05\n",
      "159 2.154839785362128e-05\n",
      "160 2.083418075926602e-05\n",
      "161 2.0143872461630963e-05\n",
      "162 1.9476628949632868e-05\n",
      "163 1.8831686247722246e-05\n",
      "164 1.8208322217105888e-05\n",
      "165 1.760570921760518e-05\n",
      "166 1.70232178788865e-05\n",
      "167 1.646017153689172e-05\n",
      "168 1.5915838957880624e-05\n",
      "169 1.5389699910883792e-05\n",
      "170 1.488106954639079e-05\n",
      "171 1.4389384887181222e-05\n",
      "172 1.391406203765655e-05\n",
      "173 1.3454545296553988e-05\n",
      "174 1.3010288057557773e-05\n",
      "175 1.2580871043610387e-05\n",
      "176 1.2165684893261641e-05\n",
      "177 1.1764303962991107e-05\n",
      "178 1.1376260772522073e-05\n",
      "179 1.100106328522088e-05\n",
      "180 1.0638386811478995e-05\n",
      "181 1.0287696568411775e-05\n",
      "182 9.948667866410688e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "eps = 1e-5\n",
    "loss = np.inf\n",
    "t = 0\n",
    "while torch.abs(global_opt - loss) > eps:\n",
    "    W = model() # W_N * W_{N - 1} * ... * W_1\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(W, X_torch, y_torch)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    # In pytorch, gradients are accumulated with .backward(), hence,\n",
    "    # we need to zero them out each round\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            param.data -= learning_rate * param.grad\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
